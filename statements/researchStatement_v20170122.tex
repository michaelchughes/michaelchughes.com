\documentclass[11pt,letterpaper]{article}
\usepackage{url}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{tgpagella}
\usepackage{parskip}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    citecolor=black,
}
\urlstyle{same}


%%% Bibliography
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{numbers,open={[},close={]}}
%\setcitestyle{authoryear,open={(},close={)}}

\title{Research Statement: Improving clinical decisions via semi-supervised Bayesian nonparametric hierarchical models}
\author{Michael C. Hughes, %\\ 
    \url{www.michaelchughes.com}
}

\begin{document}
\maketitle

My research develops machine learning
algorithms that discover interpretable clusters from large structured datasets.
Two motivating examples come from my ongoing clinical collaborations.
First, consider 
discovering common symptom trajectories for patients in the intensive care unit (ICU) and using these to better suggest 
what dose of which drug is needed.
Next, consider finding cooccurance patterns of 
symptoms and treatments
in the electronic health records (EHR) of mental health patients, to better suggest combination therapies.
A common approach to these problems is to fit hierarchical models such as the 
Latent Dirichlet Allocation 
topic model or the hidden Markov model, because these baselines offer compact and interpretable representations of the high-dimensional input data.
However, despite widespread usage these unsupervised methods often do not meet practioners' needs due to
unreliable optimization algorithms
or 
poor domain-specific predictions.
To deliver the promise of probabilistic graphical models to the broader scientific and medical community, my work seeks answers to two key questions:

\begin{itemize}

\item \textbf{Q1: How can we \emph{improve optimization algorithms} to reach good solutions consistently?} Most existing clustering methods optimize non-convex objective functions via updates to small subsets of parameters, and thus often get stuck in poor-quality local optima.
How can we escape to better solutions without complex initializations and wasteful multiple restarts?

\item \textbf{Q2: How can we \emph{incorporate domain-specific knowledge} to guide clustering?}
We care about learning representations that predict patient symptoms $x$ (observed data) as well as needed treatments $y$ (domain-specific supervision). Our clinical collaborators tell us that a method must accurately model both $x$ and $y$ to gain trust by medical professionals. Many existing models have tried, yet unfortunately end up with good $y$ predictions but poor $x$ predictions, or vice versa. Many applications offer a further challenge: the observed data $x$ is often far more plentiful than labeled outcomes $y$ (such datasets are called \emph{semi-supervised}).
\end{itemize}

As a Harvard Data Science Initiative scholar, 
I will develop novel machine learning answers to these questions.
My intended outcomes are three-fold: publications in top-tier machine learning conferences, usable open-source software,
and clinical publications.
I am particularly keen on the latter for my two ongoing collaborations with healthcare researchers:
time-series modeling of ICU patient symptoms and necessary interventions (with Marzyeh Ghassemi, Prof. Doshi-Velez, and data from Prof. Leo Celi and the MIMIC team at MIT)
and modeling EHR histories for patients with major depressive disorder (with Prof. Doshi-Velez and Dr. Roy Perlis at MGH).
Both projects provide rich datasets to explore with semi-supervised Bayesian hierarchical models. By developing reliable optimization methods to accurately model patient symptoms and needed interventions, I hope to improve clinical understanding and patient outcomes.

%First, modeling of ICU patient vital signs and 

%to model the physiological transitions of patients during their stay and suggest appropriate interventions like medication or ventilation.
%Second, with Dr. Roy Perlis at MGH, we examine the electronic health records of over 50,000 patients with major depressive disorder, hoping to identify relevant subtypes and use these to suggest targeted medications from the huge space of possible prescriptions.




\section{Past work: Reliable optimization for Bayesian nonparametrics.}

My Ph.D. thesis developed reliable optimization methods (and thus attacked Q1) for Bayesian nonparametric (BNP) clustering models.
%, particularly the Dirichlet process (DP) mixture model and its extensions. 
While parametric models require an \emph{a priori} number of clusters,
BNP models offer principled ways to 
learn model complexity from data,
balancing gains in quality from adding more clusters with a rich-get-richer preference for fewer clusters.
BNP models offer an automatic solution to the model selection problem that avoids expensive cross-validation. 
%The expected number of clusters under BNP priors grows sensibly as more data is seen. This last property is especially desirable for large-scale applications. If trained effectively, these models could keep discovering new clusters to explain new phenomena in an endless stream of data.
However, standard BNP algorithms do not fulfil this promise. Both Markov chain Monte Carlo (MCMC) methods and optimization-based variational inference use restrictive updates to subsets of variables that get stuck in poor local modes due to the limited range of each update.

Our NIPS 2013 paper \cite{hughes2013moVB} developed a new algorithm for Dirichlet process mixture models that employs proposal updates designed to jump out of local optima by adding crucial missing clusters or removing redundant or useless clusters. 
These proposals optimize a sensible variational objective function which tightly bounds the marginal likelihood and thus exhibits the ``Ockham's razor'' effect that penalizes models with too many clusters.
Furthermore, our method scales to large datasets by processing data one small batch at a time. Unlike stochastic methods that require tuning a nuisance learning rate \cite{hoffman:svi}, our scalable memoized algorithm has no learning rate at all yet guarantees that the objective will monotonically increase after every step.

% adapt the number of clusters to the provided dataset in a single training run while optimizing an objective function which tightly bounds the marginal likelihood and thus exhibits the ``Ockham's razor'' effect that penalizes models with redundant or irrelevant clusters.
%Our method escapes local optima via non-local proposal moves that can add or remove clusters to improve this objective. These proposals widen the neighborhood of possibilities that can be explored compared to earlier coordinate ascent algorithms. Furthermore, our method scales to large datasets by processing data one small batch at a time. Unlike stochastic methods that require tuning a nuisance learning rate \cite{hoffman:svi}, our scalable memoized algorithm has no learning rate at all yet guarantees that the objective will monotonically increase after every step.

Later, we extended this algorithm to topic modeling with the hierarchical Dirichlet process (HDP) \citep{hughes2015hdpreliable} as well as sequence segmentation via the HDP hidden Markov model (HDP-HMM)~\citep{hughes2015hdphmm}. 
These settings are challenging due to non-conjugacy and tighter data dependencies.
Nevertheless, our implementations optimize a variational lower bound via non-local proposal moves that scale to millions of observations.
To make these contributions accessible, I released an open-source Python package BNPy \citep{hughes2017bnpy}, which is now actively used by data science teams at the New York Times.
Looking forward, I expect this experience to directly inspire 
more work on Q1 during my postdoc, especially for more sophisticated models (like dynamic topic models) needed in EHR applications.



\section{Proposed work: Semi-supervised models for clinical time-series.}

\textbf{Multi-objective optimization for accuracy \emph{and} interpretability.}
To answer Q2 for our healthcare applications, we wish to build supervised BNP hierarchical models that jointly explain some input measurements $x$ (labs, vital signs) and some output labels $y$ (prescribed drugs or interventions).
Unlike purely supervised learning which focuses on modeling $y$ given $x$, we wish to do well simultaneously at predicting $y$ \emph{and} modeling $x$.
As a first step, we focus on topic models for bag-of-words data, where many existing flavors of supervision exist but have two key problems: either they do not consistently achieve $y$ predictions comparable to basic classifiers (see supervised LDA \citep{blei2007sLDA} and relatives), or they focus entirely on the prediction task without fitting $p(x)$, as in BP-LDA~\citep{chen2015bplda}. 
Our recent workshop paper
\citep{hughes2016clinicalSLDA} highlights failures of existing methods and shows that non-local optimization (answering Q1) will be important. Looking ahead to answer Q2, we will explore a constraint-based approach which deliberately finds the best possible model for $x$ which meets some minimum user-specified threshold on the prediction quality for $y$. We expect insights on both effective optimization (Q1) and supervision (Q2) will easily spill over from topic models into our ICU time-series applications, improving on our recent work using exclusively unsupervised features for predicting interventions like mechanical ventilation~\citep{ghassemi2017ssam}.

\textbf{Predicting combination therapies.}
Improving clinical predictions (Q2) requires principled handling of the combinatorial space of possible interventions $y$. Our collaborators at MGH have identified at least 191 drugs which are primary or auxiliary treatments for major depressive disorder. Successful treatments often require the joint prescription of two or three of these drugs, rather than just selecting one. 
Instead of naively predicting each drug's utility independently of others, we will investigate methods that would properly predict combination therapies. This can draw on advances from the structured prediction literature and use domain knowledge of drug similarity.

\textbf{Sparsity and recognition networks for extreme scalability.}
Two major challenges prevent BNP clustering from scaling to billions of examples and thousands of clusters (Q1). First, the bottleneck of training is the runtime cost of fitting a large model to new data (the ``local'' step). Recent approaches~\citep{gan2015deepTSBN,mnih2014neuralVariational} combine the fast, feed-forward properties of neural networks within an overall framework optimizing the Bayesian variational objective. By training a feed-forward network to approximate the local posterior, these methods use information from previous examples to cluster new data faster. 
Second, our recent preprint~\citep{hughes2016sparse} suggests that using sparse representations of cluster assignments can reduce storage costs and improve speed, making topic models with thousands of topics a possibility.
Incorporating such ideas together with non-local proposal moves and structured variational posteriors remains an open problem. Together, these innovations can help answer Q1 and make semi-supervised BNP optimization more effective for industry-scale problems.

%With this approach our memoized algorithm could train models with thousands of clusters. The second challenge is performing adaptive inference in a truly distributed setting, with dozens of parallel workers each creating new clusters from disjoint batches and integrating them into a coherent centralized model. Developing such a framework would make BNP clustering accessible to many industry-scale applications.


\section{Long-term Vision: Accessible tools for clinical decision-making.}

In the next decade, I think answers to Q1 and Q2 can directly help clinicians better diagnose and treat patients.
One necessary step will be integrating rich BNP representations into a reinforcement learning framework, so that we purposefully model interventions as actions that have feedback on future patient behavior, rather than just one-time outputs.
Another step will be connecting our improved optimization ideas and improved supervision ideas into probabilistic programming environments like Stan~\citep{kucukelbir2015stan}, so that scientists in many domains can try them out on new models and datasets.
I will take concrete steps towards this vision during my next few years as a postdoctoral fellow, laying the groundwork for a productive research career bridging ML and clinical care.

{\scriptsize
\setlength{\bibsep}{1pt}
\bibliography{MacrosForJournalNames,References}
}

\end{document}
