\documentclass[11pt,letterpaper,sans]{article}
\usepackage{url}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{natbib}
\usepackage{tgpagella}
\usepackage{hyperref}
\usepackage{parskip}

\title{Research Statement: Improving clinical decisions via semi-supervised Bayesian hierarchical models}
\author{Michael C. Hughes \\ www.michaelchughes.com}

\begin{document}
\maketitle

\emph{PROMPT: A statement of research interests of up to three pages that succinctly describes the applicant's research interests. The statement should explain the importance and potential impact of this research.}
\\

My research develops machine learning
algorithms that discover interpretable clusters from large datasets with hierarchical or sequential structure.
Consider applications such as 
finding patients in the intensive care unit (ICU) with similar symptom trajectories over time 
or
finding cooccurance patterns of 
failed treatments
in the electronic health records of psychiatric patients.
A common approach is to fit unsupervised hierarchical models such as the 
Latent Dirichlet Allocation 
topic model or the hidden Markov model, because these baselines offer interpretable, low-dimensional representations of the high-dimensional input data.
However, despite wide-spread usage these methods often do not meet practioners' expectations due to
unreliable optimization algorithms
or 
results which do not intuitively align
with some domain-specific knowledge not included in input data.
% to ``supervise'' the learned representations.
To deliver the promise of probabilistic graphical models to the broader scientific and medical community, my work seeks answers to two key questions:

\begin{itemize}

\item \textbf{Q1: How can we better explore 
complex posterior distributions with multiple modes?} Most existing clustering methods optimize non-convex objective functions via a local search, and thus baseline methods often get stuck in poor-quality local optima.
How can we escape to better solutions without complex initialization heuristics and wasteful multiple restarts?
Can we do better by representing multiple modes? 

\item \textbf{Q2: How can we incorporate domain-specific knowledge to guide clustering?} In the ICU, we care about learning representations that predict patient symptoms $x$ as well as needed treatments $y$. Our clinical collaborators tell us that a method must accurately model both $x$ and $y$ to gain trust by medical professionals. Many existing generative models can jointly produce $x,y$ pairs, but training methods often unfortunately end up with good $y$ predictions but poor $x$ predictions, or vice versa. Many applications offer a further challenge: the unlabeled data $x$ is often much more plentiful than outcomes data $y$.
\end{itemize}

As a Harvard Data Science Initiative scholar, 
I will develop novel machine learning answers to these questions.
My intended outcomes are three-fold: publications in top-tier machine learning conferences, usable open-source software,
and clinical publications for two ongoing collaborations with healthcare researchers: time-series modeling of ICU patient symptoms and necessary interventions (with Marzyeh Ghassemi, Prof. Leo Celi, and data from the MIMIC team at MIT)
and modeling EHR histories 
to better suggest drug combination therapies for patients with major depressive disorder (with Dr. Roy Perlis at MGH).
Both projects provide rich datasets to explore with semi-supervised Bayesian hierarchical models. By developing reliable and scalable optimization methods that escape local optima and accurately model patient symptoms and needed interventions, I hope to improve clinical understanding and patient outcomes.

%First, modeling of ICU patient vital signs and 

%to model the physiological transitions of patients during their stay and suggest appropriate interventions like medication or ventilation.
%Second, with Dr. Roy Perlis at MGH, we examine the electronic health records of over 50,000 patients with major depressive disorder, hoping to identify relevant subtypes and use these to suggest targeted medications from the huge space of possible prescriptions.




\section{Past work: Reliable optimization for Bayesian nonparametrics.}

My Ph.D. thesis project specialized in developing more robust optimization methods (and thus answering Q1) for Bayesian nonparametric (BNP) models, particularly the Dirichlet process (DP) mixture model and its extensions. 
While parametric models require the practioner to decide in advance exactly how many clusters to find in a dataset,
BNP models offer principled ways to 
learn model complexity directly from the data,
balancing gains in predictive performance from adding more clusters with a rich-get-richer preference for fewer clusters.
BNP models offer an automatic solution to the model selection problem and avoid expensive alternatives like cross-validation. 
%The expected number of clusters under BNP priors grows sensibly as more data is seen. This last property is especially desirable for large-scale applications. If trained effectively, these models could keep discovering new clusters to explain new phenomena in an endless stream of data.
However, standard BNP inference algorithms do not fulfil their promise. Both sampling-based Markov chain Monte Carlo (MCMC) methods and optimization-based variational inference are unable to explore the posterior effectively. Due to the restrictively local nature of block-coordinate ascent updates, these methods often get stuck in poor local optima.

Our NIPS 2013 paper \cite{hughes:moVB} developed a new algorithm for Dirichlet process mixture models that can adapt the number of clusters to the provided dataset in a single training run while optimizing an objective function which tightly bounds the marginal likelihood and thus exhibits the ``Ockham's razor'' effect that penalizes models with redundant or irrelevant clusters.
Our method escapes local optima via non-local proposal moves that can add or remove clusters to improve this objective. These proposals widen the neighborhood of possibilities that can be explored compared to earlier coordinate ascent algorithms. Furthermore, our method scales to large datasets by processing data one small batch at a time. Unlike stochastic methods that require tuning a nuisance learning rate \cite{hoffman:svi}, our scalable memoized algorithm has no learning rate at all yet guarantees that the objective will monotonically increase after every step.

Later, we extended this algorithm to group-wise clustering and topic modeling with the hierarchical Dirichlet process (HDP) \citep{hughes:hdpreliable} as well as sequence segmentation via the HDP hidden Markov model (HDP-HMM)~\citep{hughes:hdphmm}. In both of these settings, developing reliable proposals was challenging given non-conjugate models and tighter dependencies within the data. Nevertheless, our implementations optimize a variational lower bound which can be used for dynamic model selection while scaling to process millions of observations.
%We also demonstrated parallelization of the most expensive algorithm steps, effectively reducing the cost of training on the whole human genome from days to hours with dozens of CPU cores working together.
To make my thesis contributions accessible to the general public, I released an open-source Python package BNPy \citep{hughes2016BNPy}. This package is actively used by data science teams at the New York Times.
Looking forward, we expect lessons from designing proposals to escale local optima for BNP models to directly inspire 
more work on Q1, especially in providing theoretical guarantees on approximation quality, and extending to more sophisticated models needed in healthcare applications.



\section{Proposed work: Semi-supervised models for clinical time-series.}

\paragraph{Multi-objective optimization for accuracy \emph{and} interpretability.}
To answer Q2, we wish to build supervised BNP hierarchical models for our healthcare applications that jointly explain some input measurements $x$ (labs, vital signs) and some output labels $y$ (prescribed drugs or interventions).
Unlike purely supervised learning which focuses on modeling $y$ given $x$, we wish to do well simultaneously at predicting $y$ \emph{and} modeling the density of $x$ well.
As a first step, we focus on topic models for bag-of-words data, where many existing flavors of supervision exist but with two key problems: either they do not consistently achieve reasonable heldout predictive performance relative to simpler baseline classifiers (see supervised LDA \citep{blei2007sLDA} and relatives), or they focus entirely on the prediction task without caring at all about $p(x)$, as in BP-sLDA~\citep{chen2015bplda}. 
%Furthermore, a common symptom of many published experiments is that predictions do not improve as the model capacity of the generative model increases, as measured in the number of states or topics. 
Our recent workshop paper
\citep{hughes2016clinicalSLDA} highlights failures of existing methods and shows that effective non-local optimization (answering Q1) will be important. Looking ahead to answer Q2, we will explore a constraint-based approach which deliberately finds the best possible model for $x$ which meets some minimum specified threshold on the prediction performance for $y$. We expect insights on both effective optimization and effective supervision (Q2) will easily spill over from topic models into our ICU time-series applications.


\paragraph{Improved predictions in high-dimensional label spaces.}
An important aspect of improving supervision (Q2) for clinical applications
is dealing with hundreds of possible interventions $y$. Our collaborators at MGH have identified at least 191 drugs which are used as primary or auxiliary treatments for major depressive disorder. Successful treatments often require the joint prescription of two or three of these drugs, rather than just selecting one. 
Instead of naively predicting each drug independently of others, we propose to investigate methods that would properly predict combinations. This can draw on advances in structured prediction from the machine learning literature and domain knowledge of drug similarity.

\paragraph{Sparsity assignments and recognition networks for extreme scalability.}
Two major challenges prevent BNP clustering from scaling to billions of examples and thousands of clusters. First, the bottleneck of inference is the runtime cost of fitting a large model to new data (the ``local'' step). Recent approaches  \cite{mnih:neuralVariational} combine the fast, feed-forward properties of neural networks within an overall framework optimizing the Bayesian variational objective. By training a feed-forward network to approximate the local posterior \cite{gan:deepTSBN}, these methods use information from previous examples to cluster new data faster. 
Second, my preliminary work on sparse variational posteriors \citep{hughes2016sparse} suggests that using sparse representations of cluster assignments can reduce storage costs and improve speed, making topic models with thousands of topics a possibility. Together, these innovations could help answer Q1 and make BNP optimization much more effective for industry-scale applications.

%With this approach our memoized algorithm could train models with thousands of clusters. The second challenge is performing adaptive inference in a truly distributed setting, with dozens of parallel workers each creating new clusters from disjoint batches and integrating them into a coherent centralized model. Developing such a framework would make BNP clustering accessible to many industry-scale applications.


\section{Long-term Vision.}
\paragraph{Reinforcement learning.}
Use clustering methods above as representation strategy for RL, where actions are the prescription of different drugs, etc. 



\paragraph{Probabilistic programming.}
Probabilistic programming has come a long way in the last few years and the time is right to develop scalable inference methods that are effective for a large class of BNP models. This class could include models based on hierarchies of the Dirichlet process, the larger family of completely random measures, or latent feature models based on the Beta process. The primary challenge here is developing scalable and adaptive inference methods that are truly general purpose. Toward this end, general purpose ``black-box'' variational methods \cite{kucukelbir:Stan, ranganath:blackbox} have arisen in the past few years which could inspire new solutions when combined with the adaptive proposals from my work.


\newpage
\bibliographystyle{unsrt}
\bibliography{MacrosForJournalNames,References}

\end{document}
