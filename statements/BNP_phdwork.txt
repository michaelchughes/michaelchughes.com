As a graduate student, Mike developed improved inference algorithms for Bayesian nonparametric (BNP) clustering models. Early work focused on time series models based on the Beta process, while later work focused on the Dirichlet process (DP) mixture model and its extensions to topic models and hidden Markov models. His methods were intentionally designed to escaped poor initializations better than existing MCMC or variational algorithms, using data-driven "split" or "merge" proposals that made big changes to the proposed clusters. Without these moves, existing algorithms remain stuck in local optima with poor performance, failing to explain some data examples well and often including many redundant or overlapping clusters which hurt interpretability. An additional benefit of Mike's later work on proposal moves for variational methods is scalability: his "memoized" or "incremental" methods can process minibatches of large datasets and still guarantee monotonic improvement of an objective function.

Using these methods, Mike pursued several applications, including building large BNP topic models for every NY Times article from the last 20 years (to understand thematic trends) or training BNP hidden Markov models for chromatin segmentation of the entire human genome (to find patterns of regulation, so we understand how the body expresses some genes but inhibits others by attaching marker chemicals to DNA).
